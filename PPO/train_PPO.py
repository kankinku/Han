import gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# ğŸ§± Actor-Critic ì‹ ê²½ë§ ì •ì˜
class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        # ê³µí†µ feature ì¶”ì¶œ ê³„ì¸µ
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, 64),
            nn.ReLU()
        )
        # ì •ì±… (í–‰ë™ í™•ë¥ ) ê³„ì¸µ
        self.policy = nn.Sequential(
            nn.Linear(64, act_dim),
            nn.Softmax(dim=-1)
        )
        # ê°€ì¹˜ í•¨ìˆ˜ ê³„ì¸µ
        self.value = nn.Linear(64, 1)

    def forward(self, x):
        x = self.shared(x)
        return self.policy(x), self.value(x)

# ğŸ§® GAEë¡œ ë¦¬í„´ê³¼ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    advantage = 0
    returns = []
    advantages = []
    values = values + [0]  # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ë¥¼ 0ìœ¼ë¡œ ì¶”ê°€
    for t in reversed(range(len(rewards))):
        # Temporal difference (TD) ì˜¤ì°¨
        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]
        # GAE ëˆ„ì 
        advantage = delta + gamma * lam * (1 - dones[t]) * advantage
        returns.insert(0, advantage + values[t])  # ë¦¬í„´ = ì–´ë“œë°´í‹°ì§€ + ê°€ì¹˜
        advantages.insert(0, advantage)
    return returns, advantages

# ğŸ”’ PPO Clipped Objective ì†ì‹¤ í•¨ìˆ˜
def ppo_loss(old_log_probs, new_log_probs, advantages, clip_eps=0.2):
    # í™•ë¥  ë¹„ìœ¨ (new/old)
    ratio = torch.exp(new_log_probs - old_log_probs)
    # í´ë¦¬í•‘ëœ ì†ì‹¤
    clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages
    return -torch.min(ratio * advantages, clipped).mean()

# â–¶ï¸ í•™ìŠµ ë£¨í”„
def train():
    env = gym.make('CartPole-v1')
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n

    model = ActorCritic(obs_dim, act_dim)
    optimizer = optim.Adam(model.parameters(), lr=3e-4)

    for episode in range(1000):
        obs = env.reset()
        done = False

        # Rollout ë°ì´í„° ì €ì¥
        obs_list, action_list, reward_list = [], [], []
        logprob_list, value_list, done_list = [], [], []

        while not done:
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            probs, value = model(obs_tensor)
            dist = Categorical(probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)

            # í™˜ê²½ ìƒí˜¸ì‘ìš©
            next_obs, reward, done, _ = env.step(action.item())

            # Rollout ì €ì¥
            obs_list.append(obs_tensor)
            action_list.append(action)
            reward_list.append(reward)
            logprob_list.append(log_prob)
            value_list.append(value.item())
            done_list.append(done)

            obs = next_obs

        # GAEë¡œ ë¦¬í„´ ë° ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        returns, advantages = compute_gae(reward_list, value_list, done_list)
        advantages = torch.tensor(advantages, dtype=torch.float32)
        returns = torch.tensor(returns, dtype=torch.float32)

        # í…ì„œ ë³€í™˜
        obs_tensor = torch.stack(obs_list)
        action_tensor = torch.tensor(action_list)
        old_logprob_tensor = torch.stack(logprob_list)

        # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸ (ì—í­ ë°˜ë³µ)
        for _ in range(4):
            # forward pass
            probs, values = model(obs_tensor)
            dist = Categorical(probs)
            new_logprobs = dist.log_prob(action_tensor)

            # ì†ì‹¤ ê³„ì‚°
            pg_loss = ppo_loss(old_logprob_tensor, new_logprobs, advantages)
            vf_loss = nn.functional.mse_loss(values.squeeze(), returns)
            entropy = dist.entropy().mean()

            loss = pg_loss + 0.5 * vf_loss - 0.01 * entropy

            # ì—…ë°ì´íŠ¸
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # ë¡œê¹…
        total_reward = sum(reward_list)
        print(f"Episode {episode}, Total reward: {total_reward}")

    env.close()

# ğŸ ì‹¤í–‰
train()
